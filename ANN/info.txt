ğŸ”¹ What is a Perceptron?

A Perceptron is the basic building block of neural networks and deep learning.
It is a single artificial neuron that takes inputs, applies weights, adds a bias, and produces an output using an activation function.

ğŸ‘‰ Think of it as the simplest form of a neural network.



	â€‹

ğŸ”¹ Activation Function (Step Function)

The classic perceptron uses a binary step function:

Output = 1 â†’ Positive class

Output = 0 â†’ Negative class

This makes the perceptron suitable for binary classification.

ğŸ”¹ Working of Perceptron (Step-by-Step)

Take input values

Multiply inputs with weights

Add bias

Apply activation function

Generate output







â–¶ï¸ Forward Propagation (Prediction)

Input â†’ weights â†’ bias â†’ activation

Computes output / prediction

Moves input â†’ output

No learning happens here




â—€ï¸ Backward Propagation (Learning)

Computes gradients of loss

Uses chain rule

Moves output â†’ input

Updates weights & bias

ğŸ§  Key Differences
Forward	Backward
Prediction	Learning
Input â†’ Output	Output â†’ Input
Uses weights	Updates weights




One-Line Summary (Exam Ready)

Forward propagation predicts output, backward propagation minimizes error by updating weights using gradients.





ğŸ”¹ What is Gradient Descent?

Optimization algorithm used to minimize loss function

Updates model parameters (weights & bias)

Core learning method in Deep Learning

ğŸ”¹ Key Idea

Move parameters in the opposite direction of gradient

Gradient = slope of loss function

ğŸ”¹ Types (IMP)
Type	Description
Batch GD	Uses full dataset
Stochastic GD (SGD)	Uses one sample
Mini-batch GD	Uses small batch (most used)
ğŸ”¹ Learning Rate (Î·)

Too small â†’ slow learning

Too large â†’ divergence

Proper value â†’ fast convergence

ğŸ”¹ Role in Neural Networks

Uses gradients from backpropagation

Updates weights after each iteration/epoch

â­ One-Line Summary (Exam Ready)

Gradient Descent minimizes loss by updating parameters in the direction opposite to the gradient.